{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution\n",
    "1a) w1 = -2.04427331  w2 = 3.99683416 b = -0.92427055\n",
    "\n",
    "1b) Levenbergâ€“Marquardt as per implementation below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import scipy.optimize as optimization\n",
    "#from IPython.core.debugger import set_trace\n",
    "\n",
    "# Data\n",
    "filedir = os.path.dirname(os.path.realpath('__file__'))\n",
    "datapath = os.path.join(filedir,'assign1_data.mat')\n",
    "data = scipy.io.loadmat(datapath)\n",
    "\n",
    "# fitter wants shape (k,M) (k number of predictors)\n",
    "# data['x'] has shape (M,k) so we fix that\n",
    "x1 = data['x'][:,0]\n",
    "x2 = data['x'][:,1]\n",
    "X = np.array([x1,x2])\n",
    "# argg - shape (100,1) - gives obstuse error in the fitter\n",
    "# we fix that too (flatten)\n",
    "y = np.array(data['y']).flatten()\n",
    "\n",
    "# Initial guess\n",
    "w0 = np.array([1, 1, 1])\n",
    "\n",
    "# Objective function\n",
    "# y = w1 * x1 + w2 * x2 + b.\n",
    "def func(X, b, w1, w2):\n",
    "  # unpack independent vars\n",
    "  rval = w1*X[0] + w2*X[1] + b\n",
    "  return rval\n",
    "\n",
    "result = optimization.curve_fit(func, X, y, w0)\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import scipy.optimize as optimization\n",
    "#from IPython.core.debugger import set_trace\n",
    "\n",
    "EPS = 0.001\n",
    "N_EPOCH = 1000\n",
    "def dw(w,d,X):\n",
    "    # using numpy einstein summation to vectorize the computation\n",
    "    # vector of coefficients (d-w.x) for each pattern (alpha)     \n",
    "    C = d - np.einsum('i,ij->j', w, X) # shape is (100,)\n",
    "    # C_i * X_ji or C * transpose(X)\n",
    "    dw = EPS*np.einsum('i,ji->j', C, X) # shape is (3,)\n",
    "    return dw\n",
    "\n",
    "def main():\n",
    "    # data\n",
    "    filedir = os.path.dirname(os.path.realpath('__file__'))\n",
    "    datapath = os.path.join(filedir,'assign1_data.mat')\n",
    "    data = scipy.io.loadmat(datapath)\n",
    "    x1 = data['x'][:,0]\n",
    "    x2 = data['x'][:,1]\n",
    "    # inputs - with bias input tied high, shape is (3,100)\n",
    "    X = np.array([np.full(len(x1),1),x1,x2])\n",
    "    # output\n",
    "    d = np.array(data['y']).flatten()\n",
    "    # starting vector of weights\n",
    "    w = [1,1,1]\n",
    "\n",
    "    for _ in range(N_EPOCH):\n",
    "        w = w + dw(w,d,X)\n",
    "    print(w)\n",
    "\n",
    "# invoke main\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os as os\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import scipy.optimize as optimization\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "N_EPOCH = 30\n",
    "\n",
    "n_mis = []\n",
    "def dwp(w,d,X):\n",
    "    # using numpy einstein summation to vectorize the computation\n",
    "    # calculating w.x.d -- w.x.d < 0 => incorrect classification\n",
    "    C = d*np.einsum('i,ij->j', w, X) # shape is (100,)\n",
    "    XT = X.transpose()\n",
    "    # use enumerate for the equivalent to each_with_index (Ruby)\n",
    "    xd = [ d[i]*XT[i] for i,c in enumerate(C) if c < 0]\n",
    "    # for N vs EPOCH plot\n",
    "    n_mis.append(len(xd))\n",
    "    # sum xd element-wise\n",
    "    # dw is x.t summed over misclassified teachers\n",
    "    dw = np.einsum('ij->j',xd) # shape is (3,)\n",
    "    return dw\n",
    "\n",
    "def main():\n",
    "    # data\n",
    "    filedir = os.path.dirname(os.path.realpath('__file__'))\n",
    "    datapath = os.path.join(filedir,'assign1_data.mat')\n",
    "    data = scipy.io.loadmat(datapath)\n",
    "    x1 = data['x'][:,0]\n",
    "    x2 = data['x'][:,1]\n",
    "    # inputs - with bias input tied high, shape is (3,100)\n",
    "    X = np.array([np.full(len(x1),1),x1,x2])\n",
    "    XT = X.transpose();\n",
    "    # output\n",
    "    d = np.array(data['z']).flatten()\n",
    "    # use domain -1,1 for teachers so we can use x.w.d<0 for the classification test\n",
    "    d = [z if z == 1 else -1 for z in d]\n",
    "    # starting vector of weights\n",
    "    w = [1,1,1]\n",
    "    # sweep through the data N_EPOCH times\n",
    "    for _ in range(N_EPOCH):\n",
    "        w = w + dwp(w,d,X)\n",
    "    \n",
    "    return w\n",
    "\n",
    "# invoke main\n",
    "w_perceptron = main()\n",
    "print(w_perceptron)\n",
    "print(n_miss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of perceptron results\n",
    "\n",
    "Read the data into a panda dataframe for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def load_data():\n",
    "    csv_path = os.path.join(\".\", \"assign1_data.txt\")\n",
    "    return pd.read_csv(csv_path, sep=\"\\s+\")\n",
    "\n",
    "def load_data():\n",
    "    csv_path = os.path.join(\".\", \"assign1_data.txt\")\n",
    "    return pd.read_csv(csv_path, sep=\"\\s+\")\n",
    "\n",
    "data = load_data()\n",
    "data.head()\n",
    "c0 = data[(data.z==0)]\n",
    "c1 = data[(data.z==1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW data in parameter space with classification indicated by color\n",
    "\n",
    "The plotted line is the surface of separation given by \n",
    "x2 = -1/w2(w1x1 + w0) where w is the weight vector calculated using perceptron algorithm.  As is indicated in the figure the data is not linearly seperable so the perceptron algorithm does not converge, instead it jitters around near the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(0, 1, 128, endpoint=True)\n",
    "W = w_perceptron\n",
    "L = -(1/W[2])*(W[1]*X + W[0])\n",
    "\n",
    "ax = c0.plot.scatter(x=\"x1\", y=\"x2\", color='Red', alpha=0.4, label='C0')\n",
    "c1.plot.scatter( x=\"x1\", y=\"x2\", color='Blue', alpha=0.4, label='C1', ax=ax)\n",
    "ax.plot(X,L)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

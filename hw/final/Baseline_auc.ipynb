{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "from __future__ import print_function\n",
    "import json\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the file\n",
    "filepath = \"student_vectors_n_task_10_n_limit_10000.json\"\n",
    "student_vectors = json.load(open(filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing a mapping from qual_id and ccssm to one hot representation\n",
    "\n",
    "# collect all qual_ids, ccssm_labels, task_ids in separate lists\n",
    "all_qual_ids = []\n",
    "all_ccssm_labels = []\n",
    "all_task_ids = []\n",
    "for i in student_vectors:\n",
    "    for j in student_vectors[i]:\n",
    "        all_qual_ids.append(j['qual_id'])\n",
    "        all_ccssm_labels.append(j['ccssm'])\n",
    "        all_task_ids.append(j['task_id'])\n",
    "\n",
    "# make a set of unique values from the above lists\n",
    "unique_ids = set(all_qual_ids)\n",
    "unique_labels = set(all_ccssm_labels)\n",
    "unique_tasks = set(all_task_ids)\n",
    "# print(\"Number of unique labels in this dataset \" + str(len(unique_labels))) #unique labels\n",
    "# print(\"Number of unique lessons/tasks in this dataset \"+str(len(unique_tasks))) #unique lessons\n",
    "# print(\"Number of unique questions in this dataset \"+str(len(unique_ids))) #this is the length of bit vector (number of unique qual_ids)\n",
    "\n",
    "# generate vectors to give to fit_transform in multilabelbinarizer to further generate unique 1-hot encoding\n",
    "transform_ids = []\n",
    "for i in unique_ids:\n",
    "    transform_ids.append([i])\n",
    "\n",
    "transform_labels = []\n",
    "for i in unique_labels:\n",
    "    transform_labels.append([i])\n",
    "    \n",
    "# generate dictionary that maps labels and qual_ids to their respective 1-hot encoding\n",
    "enc = MultiLabelBinarizer()\n",
    "qual_ids_1hot = (enc.fit_transform(transform_ids)).astype(float)\n",
    "qual_ids_classes = enc.classes_\n",
    "qual_ids_dict = dict(zip(unique_ids, qual_ids_1hot))\n",
    "labels_1hot = enc.fit_transform(transform_labels).astype(float)\n",
    "labels_classes = enc.classes_\n",
    "labels_dict = dict(zip(unique_labels,labels_1hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52209\n"
     ]
    }
   ],
   "source": [
    "#preparing a mapping from qual_id and ccssm to normalized frequency distribution\n",
    "freq_dist = {}\n",
    "total_occ = {}\n",
    "total_interactions = 0\n",
    "for i in student_vectors:\n",
    "    for j in student_vectors[i]:\n",
    "        total_interactions += 1\n",
    "        if j['qual_id'] in total_occ:\n",
    "            total_occ[j['qual_id']] += 1\n",
    "        else:\n",
    "            total_occ[j['qual_id']] = 1\n",
    "        if j['ccssm'] in total_occ:\n",
    "            total_occ[j['ccssm']] += 1\n",
    "        else:\n",
    "            total_occ[j['ccssm']] = 1\n",
    "        if j['correct'] == True:\n",
    "            if j['qual_id'] in freq_dist:\n",
    "                freq_dist[j['qual_id']] += 1\n",
    "            else:\n",
    "                freq_dist[j['qual_id']] = 1\n",
    "            if j['ccssm'] in freq_dist:\n",
    "                freq_dist[j['ccssm']] += 1\n",
    "            else:\n",
    "                freq_dist[j['ccssm']] = 1\n",
    "        if j['correct'] == False: #what if all occurences were answered incorrectly\n",
    "            if j['qual_id'] not in freq_dist:\n",
    "                freq_dist[j['qual_id']] = 0\n",
    "            if j['ccssm'] not in freq_dist:\n",
    "                freq_dist[j['ccssm']] = 0\n",
    "# print(freq_dist)\n",
    "# print(\"\\n\\n\\n\")\n",
    "# print(total_occ)\n",
    "# for i in freq_dist:\n",
    "#     if freq_dist[i] != total_occ[i]:\n",
    "#         print(i)\n",
    "#         print(freq_dist[i])\n",
    "#         print(total_occ[i])\n",
    "#         print(\"\\n\")\n",
    "for i in freq_dist:\n",
    "    freq_dist[i] = float(freq_dist[i]) / float(total_occ[i])\n",
    "    \n",
    "print(total_interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct a vector where we place the frequency of a qual_id/ccssm at the position of the qual_id/ccssm in the vector.\n",
    "y_vector = np.zeros([len(unique_labels) + len(unique_ids)])\n",
    "\n",
    "for i in student_vectors:\n",
    "    for j in student_vectors[i]:\n",
    "        id_pos = np.argmax(qual_ids_dict[j['qual_id']])\n",
    "        label_pos = np.argmax(labels_dict[j['ccssm']]) + len(unique_ids)\n",
    "        if(y_vector[id_pos] == 0.0):\n",
    "            y_vector[id_pos] = freq_dist[j['qual_id']]\n",
    "        if(y_vector[label_pos] == 0.0):\n",
    "            y_vector[label_pos] = freq_dist[j['ccssm']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44758\n"
     ]
    }
   ],
   "source": [
    "# prepare labels array of size: [total_interactions, len(unique_labels) + len(unique_ids)]\n",
    "# but total_interactions should not take into account \n",
    "# first counting cases with effective interaction\n",
    "index = 0\n",
    "flag = False\n",
    "for i in student_vectors:\n",
    "    for j in student_vectors[i]:\n",
    "        if(j['correct'] == True):\n",
    "            #correct in first try\n",
    "            index += 1\n",
    "        elif(j['correct'] == False and j['second_try'] == True):\n",
    "            #false even after second try\n",
    "            index += 1\n",
    "print(index)\n",
    "effective_interactions = index\n",
    "y_tensor = np.zeros([effective_interactions, len(unique_labels) + len(unique_ids)])\n",
    "\n",
    "index = 0\n",
    "flag = False\n",
    "for i in student_vectors:\n",
    "    for j in student_vectors[i]:\n",
    "        if(j['correct'] == True):\n",
    "            #correct in first try\n",
    "            a = qual_ids_dict[j['qual_id']]\n",
    "            b = labels_dict[j['ccssm']]\n",
    "            temp = np.append(a, b)\n",
    "            y_tensor[index] = temp\n",
    "            index += 1\n",
    "        elif(j['correct'] == False and j['second_try'] == True):\n",
    "            a = qual_ids_dict[j['qual_id']]\n",
    "            b = labels_dict[j['ccssm']]\n",
    "            temp = np.append(a, b)\n",
    "            #temp = temp * (1.0/3.0)\n",
    "            temp = temp * (0.0)\n",
    "            y_tensor[index] = temp\n",
    "            index += 1\n",
    "\n",
    "# prepare predictions array of size: [effective_interactions, len(unique_labels) + len(unique_ids)]; \n",
    "pred_tensor = np.zeros([effective_interactions, len(unique_labels) + len(unique_ids)])\n",
    "for i in range(effective_interactions):\n",
    "    pred_tensor[i] = y_vector\n",
    "    \n",
    "#process y_tensor for auc\n",
    "for i in range(len(y_tensor)):\n",
    "    for j in range(len(y_tensor[0])):\n",
    "        if(y_tensor[i][j] != 0.0 and y_tensor[i][j] != 1.0):\n",
    "            print(\"hahaha\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = tf.placeholder(tf.float32, [None, len(unique_labels) + len(unique_ids)])\n",
    "# y = tf.placeholder(tf.float32, [None, len(unique_labels) + len(unique_ids)])\n",
    "# if tf.VERSION == '1.3.0':\n",
    "#     auc, opts = tf.metrics.auc(labels = y, predictions = pred, curve='ROC')\n",
    "# elif tf.VERSION == '0.12.1': #summit's tensorflow version API doc: https://www.tensorflow.org/versions/r0.12/api_docs/\n",
    "#     auc, opts = tf.contrib.metrics.streaming_auc(labels = y, predictions = pred, curve='ROC')\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     sess.run(tf.local_variables_initializer()) #https://github.com/tensorflow/tensorflow/issues/3971\n",
    "#     auc_out,opts_out = sess.run([auc,opts],feed_dict={y: y_tensor, pred: pred_tensor})\n",
    "#     print(auc_out)\n",
    "#     print(opts_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only one class present in y_true. ROC AUC score is not defined in that case.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2bcb39fc2f30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sklearn auc: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m    263\u001b[0m     return _average_binary_score(\n\u001b[1;32m    264\u001b[0m         \u001b[0m_binary_roc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/metrics/base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0my_score_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnot_average_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         score[c] = binary_metric(y_true_c, y_score_c,\n\u001b[0;32m--> 118\u001b[0;31m                                  sample_weight=score_weight)\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;31m# Average the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_binary_roc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             raise ValueError(\"Only one class present in y_true. ROC AUC score \"\n\u001b[0m\u001b[1;32m    257\u001b[0m                              \"is not defined in that case.\")\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Only one class present in y_true. ROC AUC score is not defined in that case."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "y_true = y_tensor\n",
    "y_scores = pred_tensor\n",
    "print(\"sklearn auc: {}\".format(roc_auc_score(y_true, y_scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "from __future__ import print_function\n",
    "import json\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# this code can work with either tf.VERSION = '1.3.0' or tf.VERSION = '0.12.1', functions may change for other versions\n",
    "print(tf.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading JSON file into dictionary\n",
    "filepath = \"student_vectors_n_task_10_n_limit_10000.json\"\n",
    "student_vectors = json.load(open(filepath))\n",
    "\n",
    "# examining a case with second attempt\n",
    "# for j in student_vectors['V0D6D2D5O7']:\n",
    "#     if(j['qual_id'] == '1zsCldT4p8.set2.JiTh5Wk0bD'):\n",
    "#         print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels in this dataset 4\n",
      "Number of unique lessons/tasks in this dataset 10\n",
      "Number of unique questions in this dataset 612\n"
     ]
    }
   ],
   "source": [
    "# collect all qual_ids, ccssm_labels, task_ids in separate lists\n",
    "all_qual_ids = []\n",
    "all_ccssm_labels = []\n",
    "all_task_ids = []\n",
    "for i in student_vectors:\n",
    "    for j in student_vectors[i]:\n",
    "        all_qual_ids.append(j['qual_id'])\n",
    "        all_ccssm_labels.append(j['ccssm'])\n",
    "        all_task_ids.append(j['task_id'])\n",
    "\n",
    "# make a set of unique values from the above lists\n",
    "unique_ids = set(all_qual_ids)\n",
    "unique_labels = set(all_ccssm_labels)\n",
    "unique_tasks = set(all_task_ids)\n",
    "print(\"Number of unique labels in this dataset \" + str(len(unique_labels))) #unique labels\n",
    "print(\"Number of unique lessons/tasks in this dataset \"+str(len(unique_tasks))) #unique lessons\n",
    "print(\"Number of unique questions in this dataset \"+str(len(unique_ids))) #this is the length of bit vector (number of unique qual_ids)\n",
    "\n",
    "# generate vectors to give to fit_transform in multilabelbinarizer to further generate unique 1-hot encoding\n",
    "transform_ids = []\n",
    "for i in unique_ids:\n",
    "    transform_ids.append([i])\n",
    "\n",
    "transform_labels = []\n",
    "for i in unique_labels:\n",
    "    transform_labels.append([i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dictionary that maps labels and qual_ids to their respective 1-hot encoding\n",
    "enc = MultiLabelBinarizer()\n",
    "qual_ids_1hot = (enc.fit_transform(transform_ids)).astype(float)\n",
    "qual_ids_classes = enc.classes_\n",
    "qual_ids_dict = dict(zip(unique_ids, qual_ids_1hot))\n",
    "labels_1hot = enc.fit_transform(transform_labels).astype(float)\n",
    "labels_classes = enc.classes_\n",
    "labels_dict = dict(zip(unique_labels,labels_1hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate final encoding\n",
    "final_encoding = []\n",
    "second_try_flag = False\n",
    "for i in student_vectors: #loop over all the students\n",
    "    interactions_vector = []\n",
    "    for j in student_vectors[i]: #loop over all the interactions of student 'i'\n",
    "        #assuming there is no qual_id that has 'untouched': True\n",
    "        qual_id_vector = qual_ids_dict[j['qual_id']]\n",
    "        label_vector = labels_dict[j['ccssm']]\n",
    "        combined_vector = np.concatenate([qual_id_vector,label_vector])\n",
    "        #scaling the inputs\n",
    "        if(j['correct'] == True and j['second_try'] == False):\n",
    "            #the student did correctly in the first try itself!\n",
    "            combined_vector *= (1.0)\n",
    "            interactions_vector.append(combined_vector)\n",
    "        elif(j['correct'] == False and j['second_try'] == False):\n",
    "            #student will be given second try, nothing will be appended to interactions vector\n",
    "            second_try_flag = True\n",
    "        elif(j['correct'] == True and j['second_try'] == True and second_try_flag == True):\n",
    "            #student does correctly in the second try\n",
    "            second_try_flag = False\n",
    "            combined_vector *= (2.0/3.0)\n",
    "            interactions_vector.append(combined_vector)\n",
    "        elif(j['correct'] == False and j['second_try'] == True and second_try_flag == True):\n",
    "            #student did incorrectly the second try\n",
    "            second_try_flag = False\n",
    "            combined_vector *= (1.0/3.0)\n",
    "            interactions_vector.append(combined_vector)\n",
    "    final_encoding.append(interactions_vector)\n",
    "# print(final_encoding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Students: 1255\n",
      "Maximum number of interactions (max sequence length): 177\n"
     ]
    }
   ],
   "source": [
    "# take input and output vectors from final_encoding (by the rule of o[t] = x[t+1])\n",
    "final_input_x = []\n",
    "final_output_y = []\n",
    "final_seqlen = []\n",
    "max_interactions = 0 #for max_time to give to RNN\n",
    "for i in final_encoding: #going per student\n",
    "    temp_x = []\n",
    "    temp_y = []\n",
    "    temp_max = 0\n",
    "    for j in i: #going per interaction\n",
    "        temp_x.append(j)\n",
    "        temp_y.append(j)\n",
    "        temp_max +=1\n",
    "    final_seqlen.append(temp_max)\n",
    "    if(max_interactions < temp_max):\n",
    "        max_interactions = temp_max\n",
    "    temp_x.pop() #removing last interaction that was appended to x\n",
    "    del temp_y[0] #removing the first interaction that was appended to y\n",
    "    final_input_x.append(temp_x)\n",
    "    final_output_y.append(temp_y)\n",
    "print(\"Total Number of Students: \" + str(len(final_seqlen))) #number of students\n",
    "print(\"Maximum number of interactions (max sequence length): \" + str(max_interactions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n",
      "<class 'numpy.ndarray'>\n",
      "(1130, 177, 616)\n"
     ]
    }
   ],
   "source": [
    "# converting final input to padded input (and ndarray), since we cannot feed lists to RNN since they have inconsistent dimensions.\n",
    "padded_input_x = np.zeros([len(final_seqlen), max_interactions, len(unique_ids) + len(unique_labels)])\n",
    "padded_output_y = np.zeros([len(final_seqlen), max_interactions, len(unique_ids) + len(unique_labels)])\n",
    "for i in range(len(final_input_x)):\n",
    "    for j in range(len(final_input_x[i])):\n",
    "        padded_input_x[i][j] = final_input_x[i][j]\n",
    "for i in range(len(final_output_y)):\n",
    "    for j in range(len(final_output_y[i])):\n",
    "        padded_output_y[i][j] = final_output_y[i][j]\n",
    "\n",
    "# dividing the dataset into two parts -> training (90%) and testing (10%)\n",
    "# we have data of about 1255 students as printed above. => 90% ~ 1130 students and 10% ~ 125 students\n",
    "temp_split = int(np.ceil(0.9 * len(padded_input_x)))\n",
    "print(temp_split)\n",
    "train_x = padded_input_x[:temp_split]\n",
    "train_y = padded_output_y[:temp_split]\n",
    "train_seqlen = final_seqlen[:temp_split]\n",
    "test_x = padded_input_x[temp_split:]\n",
    "test_y = padded_output_y [temp_split:]\n",
    "test_seqlen = final_seqlen[temp_split:]\n",
    "#print(type(train_x))\n",
    "#print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "learning_rate = 1.0\n",
    "display_step = 100\n",
    "n_hidden = len(unique_ids) + len(unique_labels) #number of hidden units in an RNN cell\n",
    "training_steps = 1000 #number of epochs\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, max_interactions, len(unique_ids) + len(unique_labels)]) #(<batch_size>, <max_time>, <num_features>)\n",
    "y = tf.placeholder(\"float\", [None, max_interactions, len(unique_ids) + len(unique_labels)]) #(<batch_size>, <max_time>, <num_features>)\n",
    "seqlen = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "def dynamicRNN(x,seqlen):\n",
    "    rnn_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "    outputs, states = tf.nn.dynamic_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "    return outputs\n",
    "\n",
    "pred = dynamicRNN(x, seqlen)\n",
    "\n",
    "# Define loss and optimizer\n",
    "if tf.VERSION == '1.3.0':\n",
    "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "elif tf.VERSION == '0.12.1': #summit's tensorflow version API doc: https://www.tensorflow.org/versions/r0.12/api_docs/\n",
    "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=pred, targets=y))\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x = train_x\n",
    "        batch_y = train_y\n",
    "        batch_seqlen = train_seqlen\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,seqlen: batch_seqlen})\n",
    "        test_data = test_x\n",
    "        test_label = test_y\n",
    "        #test_seqlen = test_seqlen\n",
    "        #acc_test = sess.run(accuracy, feed_dict={x: test_data, y: test_label,seqlen: test_seqlen})\n",
    "        #acc_training = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y,seqlen: batch_seqlen})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch accuracy & loss\n",
    "            acc, loss = sess.run([accuracy, cost], feed_dict={x: batch_x, y: batch_y,seqlen: batch_seqlen})\n",
    "            print(\"Step \" + str(step) + \", Loss= \" + \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \"{:.5f}\".format(acc))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate test accuracy\n",
    "    test_data = test_x\n",
    "    test_label = test_y\n",
    "    #test_seqlen = test_seqlen\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: test_data, y: test_label,seqlen: test_seqlen}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project guidelines: https://www.cs.colorado.edu/~mozer/Teaching/syllabi/DeepLearningFall2017/assignments/assignment7.html\n",
    "# Resources:\n",
    "# http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/\n",
    "# http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "# https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/dynamic_rnn.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
